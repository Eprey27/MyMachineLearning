# MyMachineLearning

https://github.com/Eprey27/MyMachineLearning/tree/master/MyMachineLearning

#Feedforward Neural Networks (FFNNs)

A feedforward neural network (FFNN) is a type of artificial neural network in which the data flows in one direction, from input to output, without looping back. The simplest form of an FFNN is a single layer perceptron, which consists of a set of inputs, a set of weights, a bias term, and a single output. The inputs are multiplied by their corresponding weights, and then the bias term is added to the sum. The final output is obtained by applying a non-linear activation function to this sum.

FFNNs can be extended to multiple layers, creating a multilayer perceptron (MLP). MLPs typically consist of an input layer, one or more hidden layers, and an output layer. Each layer is made up of a set of neurons, which are connected to the neurons in the previous and next layers. The number of neurons in each layer, as well as the number of hidden layers, can be adjusted to increase the network's representational power.

FFNNs are suitable for a wide range of tasks, such as image classification, language translation, and function approximation. They can be trained using a variety of supervised learning algorithms, such as backpropagation.

FFNNs are still popularly used today in many real-world applications, but it's worth noting that they are not very good in handling sequential data, since they don't consider the temporal dependencies and hence the recurrent neural networks (RNN) and transformer networks are more popular in handling sequential data.
